<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Shyamgopal Karthik</title>
  <link rel="shortcut icon" href="imgs/favicon/old_favicon.ico" type="image/x-icon">
  <!-- Using agarwl's CSS files -->
  <link type="text/css" rel="stylesheet" href="css/main.css">
  <link type="text/css" rel="stylesheet" href="css/fira_font.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.7.1/css/all.min.css">
  <link href='https://fonts.googleapis.com/css?family=Dosis|Raleway' rel='stylesheet' type='text/css'>

  <style type="text/css">
    /* Site title - matching agarwl's styling */
    h1,
    .site-title {
      font-family: 'Raleway', sans-serif;
      font-size: 32px;
      letter-spacing: -1px;
      text-align: center;
      padding-top: 1em;
      margin-bottom: 0.5em;
    }

    @media screen and (min-width: 550px) {

      h1,
      .site-title {
        font-size: 46px;
      }
    }

    /* Container - slightly wider */
    .container {
      max-width: 880px;
      margin: 0 auto;
      padding: 0 10px;
    }

    /* Profile layout - picture left, bio right */
    .profile-flex {
      display: flex;
      flex-direction: row;
      gap: 30px;
      align-items: flex-start;
      margin-top: 1em;
    }

    .profile-img {
      flex: 0 0 200px;
      max-width: 200px;
    }

    .profile-img img {
      width: 100%;
      max-width: 200px;
      border-radius: 8px;
    }

    .profile-info {
      flex: 1;
    }

    @media (max-width: 600px) {
      .profile-flex {
        flex-direction: column;
        align-items: center;
      }

      .profile-img {
        flex: 0 0 auto;
        max-width: 180px;
      }
    }

    /* Social links */
    .social-links {
      display: flex;
      flex-wrap: wrap;
      gap: 18px;
      justify-content: center;
      margin-bottom: 1em;
    }

    .social-links a {
      display: inline-flex;
      align-items: center;
    }

    .social-links a i {
      margin-right: 0.4em;
      font-size: 1.5em;
    }

    /* News section - simple bullets */
    .news {
      padding: 15px 0;
      margin-bottom: 10px;
    }

    .news ul {
      margin-left: 20px;
      padding-left: 0;
    }

    .news li {
      margin-bottom: 0;
    }

    /* Publication entry layout - original narrower layout */
    .pub-entry {
      display: flex;
      flex-direction: row;
      gap: 24px;
      align-items: flex-start;
      margin-bottom: 32px;
      padding: 20px;
      border-radius: 8px;
      flex-wrap: wrap;
      transition: transform 0.3s ease, box-shadow 0.3s ease;
    }

    .pub-entry:hover {
      transform: translateY(-4px);
      box-shadow: 0 8px 25px rgba(0, 0, 0, 0.15);
    }

    .pub-entry.highlight:hover {
      box-shadow: 0 8px 25px rgba(255, 193, 7, 0.25);
    }

    .pub-img {
      flex: 0 0 290px;
      max-width: 290px;
      min-width: 200px;
    }

    .pub-img img {
      width: 260px;
      height: 260px;
      object-fit: contain;
      border-radius: 8px;
      display: block;
      margin: 0 auto;
      transition: transform 0.3s ease;
    }

    .pub-entry:hover .pub-img img {
      transform: scale(1.03);
    }

    .pub-info {
      flex: 1 1 200px;
      min-width: 180px;
    }

    /* Paper title */
    .pub-info h3 {
      margin-bottom: 0.5em;
      margin-top: 0;
      font-size: 18px;
      font-weight: 400;
      line-height: 1.5;
    }

    /* Paper metadata */
    .pub-meta {
      margin-bottom: 0.5em;
      margin-top: 0;
    }

    /* Paper links */
    .pub-info>a {
      margin-right: 1em;
      margin-bottom: 0.3em;
    }

    /* Space between icon and text */
    .pub-info>a i {
      margin-right: 0.3em;
    }

    /* Highlighted papers */
    .pub-entry.highlight {
      background: #fff9e6;
    }

    /* Section headers - prominent styling */
    .pub-topic-header {
      font-size: 1.45em;
      font-weight: 600;
      color: #2a7ae2;
      margin-top: 40px;
      margin-bottom: 18px;
      letter-spacing: 0.01em;
    }

    /* Divider */
    .divider {
      border-top: 1px solid #e8e8e8;
      margin: 20px 0;
    }

    /* Mobile responsive adjustments */
    @media (max-width: 600px) {

      /* Profile: Image on top, then bio */
      .profile-flex {
        flex-direction: column-reverse;
        align-items: center;
        gap: 20px;
      }

      .profile-info {
        text-align: center;
      }

      .profile-img {
        flex: 0 0 auto;
        max-width: 180px;
        margin-bottom: 5px;
      }

      /* Publications: Stack vertically */
      .pub-entry {
        flex-direction: column;
        align-items: center;
        padding: 15px;
        margin-bottom: 25px;
      }

      .pub-img,
      .pub-info {
        flex: 0 0 100%;
        max-width: 100%;
        margin-bottom: 15px;
      }

      .pub-img img {
        /* Allow image to scale down */
        max-width: 100%;
        height: auto;
      }

      .pub-info {
        text-align: center;
        /* Center align text on mobile for cleaner look */
      }

      .pub-info>a {
        display: inline-block;
        margin-bottom: 8px;
      }

      /* Adjust header sizes */
      h1,
      .site-title {
        font-size: 28px;
        padding-top: 0.5em;
      }

      .pub-topic-header {
        margin-top: 25px;
        font-size: 1.3em;
        text-align: center;
      }
    }
  </style>
</head>

<body>

  <div class="container">
    <!-- Name at top, centered -->
    <h1>Shyamgopal Karthik</h1>

    <!-- Bio left, profile picture right -->
    <div class="profile-flex">
      <div class="profile-info">
        <p>I am a researcher at <a href="https://www.genmo.ai/">Genmo</a> working on world models. Previously, I did
          my PhD at the <a href="https://tuebingen.ai/">University of TÃ¼bingen</a> broadly working on problems at the
          intersection of Vision and Language. I've especially enjoyed worked on post-training diffusion models with
          human feedback in the past couple of years and am excited to improve the controllability and alignment of
          these models in the coming years.</p>
        <p>Before this, I completed my Bachelor's and Master's degree from the <a
            href="https://www.iiit.ac.in/">International Institute of Information Technology, Hyderabad</a>, where I
          worked with <a href="https://vineet-gandhi.github.io/">Prof. Vineet Gandhi</a> on a variety of computer vision
          problems.</p>
        <p>I've also done internships at <a href="https://research.snap.com/team/creative-vision.html">Snap Research</a>
          and <a href="https://europe.naverlabs.com/">Naver Labs Europe</a>, where I developed better preference
          optimization methods for text-to-image models and self-supervised learning methods for learning from noisy and
          imbalanced data.</p>
        <p>In a previous life, I used to be a <a href="https://ratings.fide.com/profile/25066846">terrible</a> chess
          player.</p>

        <div class="social-links">
          <a href="mailto:shyamgopalkart@gmail.com"><i class="fa-solid fa-envelope"></i>Email</a>
          <a href="https://scholar.google.com/citations?user=OiVCfscAAAAJ&hl=en"><i
              class="fa-solid fa-graduation-cap"></i>Scholar</a>
          <a href="https://x.com/ShyamgopalKart1"><i class="fa-brands fa-x-twitter"></i>Twitter</a>
          <a href="https://github.com/sgk98/"><i class="fa-brands fa-github"></i>Github</a>
          <a href="https://www.linkedin.com/in/shyamgopal-karthik-6aaa88155/"><i
              class="fa-brands fa-linkedin"></i>LinkedIn</a>
        </div>
      </div>
      <div class="profile-img">
        <img src="./imgs/Shyam_profile.jpg">
      </div>
    </div>

    <section class="news">
      <h2>News</h2>
      <ul>
        <li> 18 September 2025. <a href="https://arxiv.org/abs/2508.09968">HyperNoise</a> and <a
            href="https://arxiv.org/abs/2504.02821">SAEs for VLMs</a> were accepted at NeurIPS 2025!</li>
        <li> 1 July 2025. Joined <a href="https://www.genmo.ai/">Genmo</a> as a researcher!</li>
        <li> 25 June 2025. <a href="https://arxiv.org/abs/2410.18013">RankDPO</a> was accepted at ICCV 2025!</li>
        <li> 23 June 2025. Defended my PhD thesis!</li>
        <li>27 September 2024. <a href="https://arxiv.org/abs/2406.04312">ReNO</a> was accepted at NeurIPS 2024!</li>
        <li>27 September 2024. <a href="https://arxiv.org/abs/2407.16658">EgoCVR</a> was accepted at ECCV 2024!</li>
        <li>15 April 2024. Started my internship with Snap at Santa Monica.</li>
        <li>15 January 2024. <a href="https://arxiv.org/abs/2310.09291">CIReVL</a> was accepted at ICLR 2024!</li>
      </ul>
    </section>

    <section class="pub-section">
      <h2>Selected Publications</h2>

      <h3 class="pub-topic-header">Generative Models</h3>
      <div class="pub-list">
        <div class="pub-entry">
          <div class="pub-img">
            <img src='imgs/noise_div_spaced.png' alt='Noise Optimization Diversity publication image' height="100%"
              width="auto">
          </div>
          <div class="pub-info">
            <h3>It's Never Too Late: Noise Optimization for Collapse Recovery in Trained Diffusion Models</h3>
            <div class="pub-meta">
              <a href="https://anneharrington.github.io/">Anne Harrington</a>, <a href="https://akoepke.github.io/">A.
                Sophia Koepke</a>, <strong>Shyamgopal Karthik</strong>, <a
                href="https://people.eecs.berkeley.edu/~trevor/">Trevor Darrell</a>, <a
                href="https://people.eecs.berkeley.edu/~efros/">Alexei A. Efros</a><br>
              <em>CVPR 2026</em>.
            </div>
            <a href="https://arxiv.org/abs/2601.00090"><i class="fa-solid fa-file-lines"></i>paper</a>
            <a href="https://akoepke.github.io/noise_optimization_diversity/"><i class="fa-solid fa-globe"></i>Project
              Page</a>
            <p>Going along with my favourite topic of test-time scaling through noise optimization for diffusion models,
              we tried to implement noise optimization to increase the diversity of generated samples.
              Unsurprisingly (as I've come to see), the method worked really well. However, we found interesting
              insights through this process. First, that optimizing the eigenvalues of the similarity matrix is a much
              better way to quanitfy diversity.
              We were also able to look at the frequency of the initial noises to find improved noise initialization
              schemes in the process.
            </p>
          </div>
        </div>

        <div class="pub-entry highlight">
          <div class="pub-img">
            <img src='imgs/hypernoise.png' alt='RankDPO publication image' height="100%" width="auto">
          </div>
          <div class="pub-info">
            <h3>Noise Hypernetworks: Amortizing Test-Time Compute in Diffusion Models</h3>
            <div class="pub-meta">
              <a href="https://lucaeyring.com/">Luca Eyring </a>, <strong>Shyamgopal Karthik</strong>, <a
                href="https://scholar.google.com/citations?user=FXNJRDoAAAAJ&hl=en">Alexey Dosovitskiy</a>,<a
                href="https://natanielruiz.github.io/">Nataniel Ruiz*</a>, <a
                href="https://www.eml-unitue.de/people/zeynep-akata">Zeynep Akata*</a><br>
              <em>NeurIPS 2025</em>.
            </div>
            <a href="https://arxiv.org/abs/2508.09968"><i class="fa-solid fa-file-lines"></i>paper</a>
            <a href="https://github.com/ExplainableML/HyperNoise"><i class="fa-brands fa-github"></i>code</a>
            <a href="https://noisehypernetworks.github.io/"><i class="fa-solid fa-globe"></i>Project Page</a>
            <p>Test-time scaling had become ubiquitous in text-to-image models (among other things due to our work).
              However, we found a neat way to explicitly post-train for the test-time noise optimization objective by
              training a "hypernetwork" that is optimized to predict the optimal initial noise. This gave neat
              improvements even on large models like FLUX, but also gives a neat formulation for reward optimization
              with distilled models.
            </p>
          </div>
        </div>

        <div class="pub-entry highlight">
          <div class="pub-img">
            <img src='imgs/rankdpo.png' alt='RankDPO publication image' height="100%" width="auto">
          </div>
          <div class="pub-info">
            <h3>Scalable Ranked Preference Optimization for Text-to-Image Generation</h3>
            <div class="pub-meta">
              <strong>Shyamgopal Karthik</strong>, <a
                href="https://scholar.google.com/citations?user=nwjxmycAAAAJ&hl=en">Huseyin Coskun</a>, <a
                href="https://www.eml-unitue.de/people/zeynep-akata">Zeynep Akata</a>, <a
                href="https://stulyakov.com/">Sergey Tulyakov</a>, <a href="https://alanspike.github.io/">Jian Ren</a>,
              and <a href="https://scholar.google.com/citations?user=bZdVsMkAAAAJ">Anil Kag</a><br>
              <em>ICCV 2025</em>.
            </div>
            <a href="https://arxiv.org/abs/2410.18013"><i class="fa-solid fa-file-lines"></i>paper</a>
            <a href="https://snap-research.github.io/RankDPO/"><i class="fa-solid fa-globe"></i>Project Page</a>
            <p>While ReNO did an amazing job at improving the quality of text-to-image models, this came with an
              increased runtime. As a result, we were looking at DPO based techniques to improve the quality of
              text-to-image models. Turns out, the biggest bottleneck with applying DPO on these models is that public
              datasets for these tasks aren't of great quality. To address this issue, we generated and labelled a
              <strong>new preference dataset</strong> using newer text-to-image models and off-the-shelf reward models.
              This also allowed us to collect preference rankings and develop a nice <strong>ranking based
                objective</strong> to improve upon the standard DPO objective.
            </p>
          </div>
        </div>

        <div class="pub-entry highlight">
          <div class="pub-img">
            <img src='imgs/reno.png' alt='ReNO publication image' height="100%" width="auto">
          </div>
          <div class="pub-info">
            <h3>ReNO: Enhancing One-step Text-to-Image Models through Reward-based Noise Optimization</h3>
            <div class="pub-meta">
              <a href="https://lucaeyring.com/">Luca Eyring*</a>, <strong>Shyamgopal Karthik*</strong>, <a
                href="https://karroth.com/">Karsten Roth</a>, <a
                href="https://scholar.google.com/citations?user=FXNJRDoAAAAJ&hl=en">Alexey Dosovitskiy</a>, <a
                href="https://www.eml-unitue.de/people/zeynep-akata">Zeynep Akata</a><br>
              <em>NeurIPS 2024</em>, Vancouver, Canada.
            </div>
            <a href="https://arxiv.org/abs/2406.04312"><i class="fa-solid fa-file-lines"></i>paper</a>
            <a href="https://github.com/ExplainableML/ReNO"><i class="fa-brands fa-github"></i>code</a>
            <a href="https://huggingface.co/spaces/fffiloni/ReNO"><i class="fa-solid fa-play-circle"></i>demo</a>
            <p>We knew that best-of-n sampling with a reward model was already an extremely strong baseline. However,
              could we go one-step further and <strong>optimize the initial noise</strong> to improve this further? This
              problem stumped us for a long while since backprop through the whole diffusion process was expensive and
              had exploding gradients. We finally found the solution with <strong>one-step models</strong>! However,
              would the one-step models be good enough to work with? Turns out that optimziing the noise of one-step
              text-to-image models could give us results that were competitive with proprietary closed source models
              that were 10x larger! This also culminated a frutiful 1.5 year journey for me of trying my best to find
              interesting research directions without updating a <strong>single</strong> parameter of any model.
            </p>
          </div>
        </div>

        <div class="pub-entry">
          <div class="pub-img">
            <img src='imgs/imageselect.png' alt='ImageSelect publication image' height="100%" width="auto">
          </div>
          <div class="pub-info">
            <h3>If at First You Don't Succeed, Try, Try Again: Faithful Diffusion-based Text-to-Image Generation by
              Selection</h3>
            <div class="pub-meta">
              <strong>Shyamgopal Karthik*</strong>, <a href="https://karroth.com/">Karsten Roth*</a>, <a
                href="https://mancinimassimiliano.github.io/">Massimiliano Mancini</a>, <a
                href="https://www.eml-unitue.de/people/zeynep-akata">Zeynep Akata</a><br>
              <em>ICCV Workshop on Multimodal Foundation Models 2023</em>, Paris, France.
            </div>
            <a href="https://arxiv.org/abs/2305.13308"><i class="fa-solid fa-file-lines"></i>paper</a>
            <a href="https://github.com/ExplainableML/ImageSelect"><i class="fa-brands fa-github"></i>code</a>
            <p>This paper started my journey into text-to-image generation. The main challenge we had was that Stable
              Diffusion models were doing a decent job at generating high-quality images, but there were tons of issues
              in closely following the prompt. While there were several methods proposed especially focusing on the
              attention maps during inference, we realized than best-of-n sampling with a human-preference reward model
              went a long way in improving the results. While this was quite trivial in some ways, it set the stage for
              us to continue exploring the effectiveness of reward models and the effect of the seed in image
              generation.
            </p>
          </div>
        </div>
      </div>

      <h3 class="pub-topic-header">Compositionality and VLMs</h3>

      <div class="pub-entry">
        <div class="pub-img">
          <img src='imgs/Introduction_figure_dino.jpg' alt='SAE-VLM publication image' height="100%" width="auto">
        </div>
        <div class="pub-info">
          <h3>Sparse Autoencoders Learn Monosemantic Features in Vision-Language Models</h3>
          <div class="pub-meta">
            <a href="https://scholar.google.com/citations?hl=en&user=s5FT5bIAAAAJ">Mateusz Pach</a>, <strong>Shyamgopal
              Karthik</strong>, <a href="https://qbouniot.github.io/">Qunetin Bouniot</a>, <a
              href="https://sergebelongie.github.io/"> Serge Belongie</a>, <a
              href="https://www.eml-unitue.de/people/zeynep-akata">Zeynep Akata</a><br>
            <em>NeurIPS 2025</em>.
          </div>
          <a href="https://arxiv.org/abs/2504.02821"><i class="fa-solid fa-file-lines"></i>paper</a>
          <a href="https://github.com/ExplainableML/sae-for-vlm"><i class="fa-brands fa-github"></i>code</a>
          <p>This was an illuminating exploration into mechanistic interpretability and sparse Autoencoders for me.
            Being able to steer multimodal LLMs with just an SAE on the vision encoder without touching the language
            decoder in an unsupervised manner was especially impressive result.
            However, as people are finding out, SAEs (much like other interpretability methods) don't seem to be the
            panacea for the difficult problems of safety and controllability.
          </p>
        </div>
      </div>

      <div class="pub-list">
        <div class="pub-entry">
          <div class="pub-img">
            <img src='imgs/good_crepe.jpg' alt='Good CREPE publication image' height="100%" width="auto">
          </div>
          <div class="pub-info">
            <h3>A Good CREPE needs more than just Sugar: Investigating Biases in Compositional Vision-Language
              Benchmarks</h3>
            <div class="pub-meta">
              <a href="https://vishaal27.github.io/">Vishaal Udandarao*</a>, <a href="https://mehdidc.github.io/">Mehdi
                Cherti*</a>, <strong>Shyamgopal Karthik</strong>, <a
                href="https://scholar.google.de/citations?user=p1FuAMkAAAAJ&hl=en">Jenia Jitsev</a>, <a
                href="https://samuelalbanie.com/">Samuel Albanie</a>, <a
                href="https://scholar.google.com/citations?user=0z0fNxUAAAAJ&hl=en">Matthias Bethge</a><br>
              <em>EVAL-FOMO Workshop at CVPR 2025</em>.
            </div>
            <a href="https://arxiv.org/abs/2506.08227"><i class="fa-solid fa-file-lines"></i>paper</a>
            <p>Along with <a href="https://vishaal27.github.io/">Vishaal</a> and <a
                href="https://mehdidc.github.io/">Mehdi</a>, we had noticed that VLM benchmarks had a ton of issues,
              especially the ones focusing on fine-grained, compositional tasks. This paper was an opportunity for us to
              show the bitter lesson of vision-language benchmarks, where blind baselines and heuristics outperform VLMs
              in several cases, and fixing these benchmarks isn't too straightforward either. A few months after putting
              this paper out, we found out similar <a href="https://arxiv.org/abs/2511.16655">issues</a> with the
              benchmarks in <a href="https://arxiv.org/abs/2511.04670">Cambrian-S</a> too.</p>
          </div>
        </div>

        <div class="pub-entry">
          <div class="pub-img">
            <img src='imgs/egocvr.png' alt='EgoCVR publication image' height="100%" width="auto">
          </div>
          <div class="pub-info">
            <h3>EgoCVR: An Egocentric Benchmark for Fine-Grained Composed Video Retrieval</h3>
            <div class="pub-meta">
              <a href="https://hummelth.github.io/">Thomas Hummel*</a>, <strong>Shyamgopal Karthik*</strong>, <a
                href="https://lilygeorgescu.github.io/">Mariana-Iuliana Georgescu</a>, <a
                href="https://www.eml-unitue.de/people/zeynep-akata">Zeynep Akata</a><br>
              <em>ECCV 2024</em>, Milan, Italy.
            </div>
            <a href="https://arxiv.org/abs/2407.16658"><i class="fa-solid fa-file-lines"></i>paper</a>
            <a href="https://github.com/ExplainableML/EgoCVR"><i class="fa-brands fa-github"></i>code</a>
            <p>Building on our previous work, we were keen on exploring Composed Video Retrieval. The biggest issue was
              that the existing benchmark (WebVid-CoVR) was focused excessively on images and did not really require the
              whole video to solve the task. To address this issue, we spent a lot of time manually curating a nice
              evaluation set from Ego4D which eventually turned out into a very nice benchmark. CIReVL adapted for
              videos also turned out to be a very nice training-free method that was competitive with mehtods training
              on millions of videos!
            </p>
          </div>
        </div>

        <div class="pub-entry highlight">
          <div class="pub-img">
            <img src='imgs/CIReVL-Full.png' alt='Vision-by-Language publication image' height="100%" width="auto">
          </div>
          <div class="pub-info">
            <h3>Vision-by-Language for Training-Free Compositional Image Retrieval</h3>
            <div class="pub-meta">
              <strong>Shyamgopal Karthik*</strong>, <a href="https://karroth.com/">Karsten Roth*</a>, <a
                href="https://mancinimassimiliano.github.io/">Massimiliano Mancini</a>, <a
                href="https://www.eml-unitue.de/people/zeynep-akata">Zeynep Akata</a><br>
              <em>ICLR 2024</em>, Vienna, Austria.
            </div>
            <a href="https://arxiv.org/abs/2310.09291"><i class="fa-solid fa-file-lines"></i>paper</a>
            <a href="https://github.com/ExplainableML/Vision_by_Language"><i class="fa-brands fa-github"></i>code</a>
            <p>We started off looking at Composed Image Retrieval task where we have a query image and textual
              instruction that modified the query. Popular methods for this task were trained similar to textual
              inversion methods and predicted a "pseudo-token" for the query image. Our immediate instinct was that
              using an off-the-shelf captioning model <strong> must </strong> provide a stronger and more interpretable
              signal than these trained pseudo-token methods. Therefore, our "vision-by-language" method was just to
              caption an image, reformulate the caption based on the textual instruction and retrieve images based on
              the reformulated caption. Not only was this method more interpretable and training-free, it also allowed
              us to <strong>double</strong> the state-of-the-art performance on some popular benchmarks.
            </p>
          </div>
        </div>
      </div>

      <h3 class="pub-topic-header">Representation Learning</h3>
      <div class="pub-list">
        <div class="pub-entry">
          <div class="pub-img">
            <img src='imgs/kgsp.png' alt='KG-SP publication image' height="100%" width="auto">
          </div>
          <div class="pub-info">
            <h3>KG-SP: Knowledge Guided Simple Primitives for Open World Compositional Zero-Shot Learning</h3>
            <div class="pub-meta">
              <strong>Shyamgopal Karthik</strong>, <a href="https://mancinimassimiliano.github.io/">Massimiliano
                Mancini</a>, <a href="https://www.eml-unitue.de/people/zeynep-akata">Zeynep Akata</a><br>
              <em>CVPR 2022</em>.
            </div>
            <a
              href="https://openaccess.thecvf.com/content/CVPR2022/papers/Karthik_KG-SP_Knowledge_Guided_Simple_Primitives_for_Open_World_Compositional_Zero-Shot_CVPR_2022_paper.pdf"><i
                class="fa-solid fa-file-lines"></i>paper</a>
            <a href="https://github.com/ExplainableML/KG-SP"><i class="fa-brands fa-github"></i>code</a>
            <p>In this work, we looked at the problem of Compositional Zero-Shot Learning, where the goal is to predict
              (attribute, object) labels for an image, and generalize to unseen (attribute, object) pairs. Recent
              methods had tried to model attributes and objects jointly using a variety of ideas. Here, we show that
              predicting attributes and objects independently can work quite well for this task. Additionally, we show
              how a knowledge-base can be incorporated to improve the performance of the model at inference. Finally, we
              introduce a new partially labeled setting where we show how we can train our model in the absence of
              compositional labels.
            </p>
          </div>
        </div>

        <div class="pub-entry">
          <div class="pub-img">
            <img src='imgs/ssl_big.png' alt='SSL publication image' width="auto" height="100%">
          </div>
          <div class="pub-info">
            <h3>Learning from Long-Tailed Data with Noisy Labels</h3>
            <div class="pub-meta">
              <strong>Shyamgopal Karthik</strong>, <a
                href="https://europe.naverlabs.com/people_user/Jerome-Revaud/">Jerome Revaud</a>, <a
                href="https://europe.naverlabs.com/people_user/Boris-Chidlovskii/">Boris Chidlovskii</a><br>
              <em>ICCV 2021 Workshop on Self-supervised Learning for Next-Generation Industry-level Autonomous
                Driving</em>.
            </div>
            <a href="https://arxiv.org/abs/2108.11096"><i class="fa-solid fa-file-lines"></i>paper</a>
            <p>This paper started off as a journey towards developing methods that are robust to both label noise
              and long-tailed class distributions. Methods tailored for one of these challenges collapsed when the other
              challenge was introduced. In the end, it turned out that vanilla self-supervised training went a long way
              in learning representations that were robust to both label noise and long-tailed distributions.
            </p>
          </div>
        </div>

        <div class="pub-entry">
          <div class="pub-img">
            <img src='imgs/iclr.jpg' alt='ICLR publication image' width="200">
          </div>
          <div class="pub-info">
            <h3>No Cost Likelihood Manipulation at Test Time for Making Better Mistakes in Deep Networks</h3>
            <div class="pub-meta">
              <strong>Shyamgopal Karthik</strong>, <a href="https://drimpossible.github.io/">Ameya Prabhu</a>, <a
                href="https://puneetkdokania.github.io/">Puneet Dokania</a>, <a
                href="https://faculty.iiit.ac.in/~vgandhi/">Vineet Gandhi</a><br>
              <em> ICLR 2021</em>.
            </div>
            <a href="https://arxiv.org/abs/2104.00795"><i class="fa-solid fa-file-lines"></i>paper</a>
            <a href="https://github.com/sgk98/CRM-Better-Mistakes"><i class="fa-brands fa-github"></i>code</a>
            <p>The main motivation behind this work was to see if we could reduce the severity of mistakes in a
              classification setting. To do this, we make use of label hierarchies which are readily available through
              taxonomies like WordNet. For our method, we show that a simple algorithm from Duda and Hart's Pattern
              Recognition textbook way back in 1973 can be effectively used in a post-hoc manner while retaining the
              calibration of the base model.
            </p>
          </div>
        </div>

        <div class="pub-entry">
          <div class="pub-img">
            <img src='imgs/tracking.jpg' alt='Tracking publication image' width="200">
          </div>
          <div class="pub-info">
            <h3>Simple Unsupervised Multi-Object Tracking</h3>
            <div class="pub-meta">
              <strong>Shyamgopal Karthik</strong>, <a href="https://drimpossible.github.io">Ameya Prabhu</a>, <a
                href="https://faculty.iiit.ac.in/~vgandhi">Vineet Gandhi</a> <br>
              <em> Arxiv 2020</em>.
            </div>
            <a href="https://arxiv.org/abs/2006.02609"><i class="fa-solid fa-file-lines"></i>paper</a>
            <p>We revisited Re-Idenification models that were widely used in Multi-Object Tracking algorithms. In
              various trackers, this is often the only component that requires video level supervision. Our insight was
              that we could train a ReID model using pseudo-labels generated from a Kalman filter based tracker in a
              self-supervised fashion. The resulting ReID model can be used as a drop-in replacement to the supervised
              ReID models used in trackers. Alternatively, using these ReID features as a post-processing step in
              trackers that don't use a ReID model can reduce the number of ID Switches by 67%. In hindsight, I hope
              this was useful in motivating some <a href="https://arxiv.org/abs/2206.04656">works</a> later on which
              strengthened the traditional tracking-by-detection paradigm.
          </div>
        </div>
      </div>
  </div>
  </section>
  </div>

  </div>
  </div>


</body>

</html>