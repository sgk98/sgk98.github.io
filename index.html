<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
  <head>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
    /* Color scheme stolen from Sergey Karayev */
    a {
    color: #1772d0;
    text-decoration:none;
    }
    a:focus, a:hover {
    color: #f09228;
    text-decoration:none;
    }
    body,td,th,tr,p,a {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px
    }
    strong {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px;
    }
    heading {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 22px;
    }
    papertitle {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 16px;
    font-weight: 700
    }
    name {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 32px;
    }
    .one
    {
    width: 160px;
    height: 160px;
    position: relative;
    }
    .two
    {
    width: 160px;
    height: 160px;
    position: absolute;
    transition: opacity .2s ease-in-out;
    -moz-transition: opacity .2s ease-in-out;
    -webkit-transition: opacity .2s ease-in-out;
    }
    .fade {
     transition: opacity .2s ease-in-out;
     -moz-transition: opacity .2s ease-in-out;
     -webkit-transition: opacity .2s ease-in-out;
    }
    span.highlight {
        background-color: #ffffd0;
    }
    .profile-flex {
      display: flex;
      flex-direction: row;
      gap: 32px;
      align-items: center;
      justify-content: space-between;
      flex-wrap: wrap;
    }
    .profile-info {
      flex: 2 1 300px;
      min-width: 250px;
    }
    .profile-img {
      flex: 1 1 160px;
      min-width: 120px;
      max-width: 200px;
      display: flex;
      align-items: center;
      justify-content: center;
    }
    .profile-img img {
      width: 100%;
      border-radius: 50%;
    }
    .container {
      max-width: 800px;
      margin: 0 auto;
      padding: 0 10px;
    }
    .news {
      padding: 20px 0;
    }
    .news heading {
      display: block;
      margin-bottom: 8px;
    }
    .pub-section {
      margin-top: 32px;
    }
    .pub-section heading {
      display: block;
      margin-bottom: 8px;
    }
    .pub-entry {
      display: flex;
      flex-direction: row;
      gap: 24px;
      align-items: flex-start;
      margin-bottom: 32px;
      padding: 20px;
      border-radius: 8px;
      flex-wrap: wrap;
    }
    .pub-entry.highlight {
      background: #ffffd0;
    }
    .pub-img {
      flex: 0 0 160px;
      max-width: 160px;
      min-width: 100px;
      display: flex;
      align-items: center;
      justify-content: center;
    }
    .pub-img img {
      width: 120px;
      height: 120px;
      object-fit: cover;
      border-radius: 8px;
      background: #fff;
      display: block;
      margin: 0 auto;
    }
    .pub-info {
      flex: 1 1 200px;
      min-width: 180px;
    }
    @media (max-width: 800px) {
      .profile-flex {
        flex-direction: column;
        gap: 16px;
      }
      .pub-entry {
        flex-direction: column;
        gap: 12px;
      }
      .container {
        padding: 0 4px;
      }
      .pub-img img {
        width: 90px;
        height: 90px;
      }
    }
  </style>
  <title>Shyamgopal Karthik</title>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="mystylesheet" type="text/css" href="mystylesheet.css">
  <link rel="shortcut icon" href="imgs/favicon/favicon.ico" type="image/x-icon">
  <link href='http://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons/font/bootstrap-icons.css">
      <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.7.1/css/all.min.css">
      <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lexend">
      <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato">
      <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Mulish">
  </head>

  <body>
  <div class="container">
    <div class="profile-flex" style="margin-top: 24px;">
      <div class="profile-info">
        <h1 style="text-align:center; font-weight:bold; font-size:2.5em;">Shyamgopal Karthik</h1>
        <p>I am a final year PhD candidate at the University of TÃ¼bingen. In the past few years, I've broadly worked on problems at the intersection of Vision and Language. In the past year, I've especially enjoyed worked on enhancing diffusion models with human feedback. I'm excited to develop better controllability and alignment of image and video models in the coming years. <br/> <br/>

        Before this, I completed my Bachelor's and Master's degree from the <a href="https://www.iiit.ac.in/">International Institute of Information Technology, Hyderabad</a> in 2021 where I worked with on a variety of computer vision problems. <br/> <br/>
        Recently, I also had the pleasure of doing an internship at the <a href="https://research.snap.com/team/creative-vision.html">Creative Vision</a> team at Snap Research in Santa Monica where I worked on improving Direct Preference Optimization for text-to-image models. Previously, I did an internship at <a href="https://europe.naverlabs.com/">Naver Labs Europe</a>, using self-supervised learning methods to learn from noisily labeled data. <br/> <br/>
        In a previous life, I used to be a <a href="https://ratings.fide.com/profile/25066846">terrible</a> chess player.
          <br/>
          <br/>
</p>

  <p>
    </p>
<!--         <p align=center>
          <a href="mailto: shyamgopalkart@gmail.com">Email</a> &nbsp/&nbsp
          <a href="./pdfs/CV_Shyam.pdf">CV</a> &nbsp/&nbsp
          <a href="https://scholar.google.co.in/citations?user=OiVCfscAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
    <a href="https://github.com/sgk98/">GitHub</a> &nbsp/&nbsp
            <a href="https://twitter.com/ShyamgopalKart1">Twitter</a>
        </p> -->
        <p align=center>
                 <a href="mailto:shyamgopalkart@gmail.com"><i class="fa-solid fa-envelope"></i> Email</a> &nbsp;/&nbsp;
                 <a href="https://scholar.google.com/citations?user=OiVCfscAAAAJ&hl=en"><i class="fa-solid fa-graduation-cap"></i> Scholar</a> &nbsp;/&nbsp;
                 <a href="https://x.com/ShyamgopalKart1"><i class="fa-brands fa-x-twitter"></i> Twitter</a> &nbsp;/&nbsp;
                 <a href="https://github.com/sgk98/"><i class="fa-brands fa-github"></i> Github</a> &nbsp;/&nbsp;
                 <a href="https://bsky.app/profile/shyamgopal.bsky.social"><i class="fa-brands fa-bluesky"></i> Bluesky</a>
        </p>
        </div>
        <div class="profile-img">
          <img width="100%" style="border-radius: 50%" src="./imgs/Shyam_profile.jpg">
        </div>
      </div>
      <section class="news">
        <h2>News</h2>
        <p>
          <ul>
            <li>27 September 2024. <a href="https://arxiv.org/abs/2406.04312">ReNO</a> was accepted to NeurIPS 2024!</li>
            <li>27 September 2024. <a href="https://arxiv.org/abs/2407.16658">EgoCVR</a> was accepted to ECCV 2024!</li>
            <li>15 April 2024. Started my internship with Snap at Santa Monica.</li>
            <li>27 September 2024. <a href="https://arxiv.org/abs/2310.09291">CIReVL</a> was accepted to ICLR 2024!</li>
<!--                   <li>19 October 2022. I was recognized as an outstanding reviewer at <a href="https://eccv2022.ecva.net/program/outstanding-reviewers/">ECCV 2022</a></li>
                  <li>03 July 2022. Our work on post-hoc uncertainty estimation was accepted to <a href="https://eccv2022.ecva.net/">ECCV 2022</a></li>
                  <li>21 May 2022. I was recognized as an outstanding reviewer at <a href="https://cvpr2022.thecvf.com/outstanding-reviewers">CVPR 2022</a>.</li>
                  <li>3 March 2022. Our work on Compositional Zero-Shot Learning was accepted at <a href="https://cvpr2022.thecvf.com/">CVPR 2022</a></li>
                  <li>1 October 2021. Started my PhD at the <a href="https://www.eml-unitue.de/"> Explainable Machine Learning</a> group. </li> -->
          </ul>
        </p>
      </section>

      <section class="pub-section">
        <h2>Selected Publications</h2>
      </section>

      <div class="pub-list">
        <div class="pub-entry highlight">
          <div class="pub-img">
            <img src='imgs/rankdpo.png' alt='RankDPO publication image' height="100%" width="auto">
          </div>
          <div class="pub-info">
            <h3>Scalable Ranked Preference Optimization for Text-to-Image Generation</h3>
            <br>
            <strong>Shyamgopal Karthik</strong>, <a href="https://scholar.google.com/citations?user=nwjxmycAAAAJ&hl=en">Huseyin Coskun</a>, <a href="https://www.eml-unitue.de/people/zeynep-akata">Zeynep Akata</a>, <a href="https://stulyakov.com/">Sergey Tulyakov</a>, <a href="https://alanspike.github.io/">Jian Ren</a>, and <a href="https://scholar.google.com/citations?user=bZdVsMkAAAAJ">Anil Kag</a><br>
            <em>Arxiv 2024</em> <br><a href="https://arxiv.org/abs/2410.18013"><i class="fa-solid fa-file-lines"></i>&nbsp;paper</a>&nbsp;|&nbsp;<a href="https://snap-research.github.io/RankDPO/"><i class="fa-solid fa-globe"></i>&nbsp;webpage</a>&nbsp;|&nbsp;<a href="newbibs/rankdpo.bib"><i class="fa-solid fa-book"></i>&nbsp;bibtex</a>
            <p>While ReNO did an amazing job at improving the quality of text-to-image models, this came with an increased runtime. As a result, we were looking at DPO based techniques to improve the quality of text-to-image models. Turns out, the biggest bottleneck with applying DPO on these models is that public datasets for these tasks aren't of great quality. To address this issue, we generated and labelled a <strong>new preference dataset</strong> using newer text-to-image models and off-the-shelf reward models. This also allowed us to collect preference rankings and develop a nice <strong>ranking based objective</strong> to improve upon the standard DPO objective. 
            </p>
          </div>
        </div>

        <div class="pub-entry highlight">
          <div class="pub-img">
            <img src='imgs/reno.png' alt='ReNO publication image' height="100%" width="auto">
          </div>
          <div class="pub-info">
            <h3>ReNO: Enhancing One-step Text-to-Image Models through Reward-based Noise Optimization</h3>
            <br>
            <a href="https://lucaeyring.com/">Luca Eyring*</a>, <strong>Shyamgopal Karthik*</strong>, <a href="https://karroth.com/">Karsten Roth</a>, <a href="https://scholar.google.com/citations?user=FXNJRDoAAAAJ&hl=en">Alexey Dosovitskiy</a>, and <a href="https://www.eml-unitue.de/people/zeynep-akata">Zeynep Akata</a><br>
            <em>NeurIPS 2024</em>, Vancouver, Canada.<br><a href="https://arxiv.org/abs/2406.04312"><i class="fa-solid fa-file-lines"></i>&nbsp;paper</a>&nbsp;|&nbsp;<a href="https://github.com/ExplainableML/ReNO"><i class="fa-brands fa-github"></i>&nbsp;code</a>&nbsp;|&nbsp;<a href="https://huggingface.co/spaces/fffiloni/ReNO"><i class="fa-solid fa-play-circle"></i>&nbsp;demo</a>&nbsp;|&nbsp;<a href="newbibs/reno.bib"><i class="fa-solid fa-book"></i>&nbsp;bibtex</a>
            <p>We knew that best-of-n sampling with a reward model was already an extremely strong baseline. However, could we go one-step further and <strong>optimize the initial noise</strong> to improve this further? This problem stumped us for a long while since backprop through the whole diffusion process was expensive and had exploding gradients. We finally found the solution with <strong>one-step models</strong>! However, would the one-step models be good enough to work with? Turns out that optimziing the noise of one-step text-to-image models could give us results that were competitive with proprietary closed source models that were 10x larger! This also culminated a frutiful 1.5 year journey for me of trying my best to find interesting research directions without updating a <strong>single</strong> parameter of any model.
            </p>
          </div>
        </div>

        <div class="pub-entry">
          <div class="pub-img">
            <img src='imgs/imageselect.png' alt='ImageSelect publication image' height="100%" width="auto">
          </div>
          <div class="pub-info">
            <h3>If at First You Don't Succeed, Try, Try Again: Faithful Diffusion-based Text-to-Image Generation by Selection</h3>
            <br>
            <strong>Shyamgopal Karthik*</strong>, <a href="https://karroth.com/">Karsten Roth*</a>, <a href="https://mancinimassimiliano.github.io/">Massimiliano Mancini</a>, and <a href="https://www.eml-unitue.de/people/zeynep-akata">Zeynep Akata</a><br>
            <em>ICCV Workshop on Multimodal Foundation Models 2023</em>, Paris, France.<br><a href="https://arxiv.org/abs/2305.13308"><i class="fa-solid fa-file-lines"></i>&nbsp;paper</a>&nbsp;|&nbsp;<a href="https://github.com/ExplainableML/ImageSelect"><i class="fa-brands fa-github"></i>&nbsp;code</a>&nbsp;|&nbsp;<a href="newbibs/imageselect.bib"><i class="fa-solid fa-book"></i>&nbsp;bibtex</a>
            <p>This paper started my journey into text-to-image generation. The main challenge we had was that Stable Diffusion models were doing a decent job at generating high-quality images, but there were tons of issues in closely following the prompt. While there were several methods proposed especially focusing on the attention maps during inference, we realized than best-of-n sampling with a human-preference reward model went a long way in improving the results. While this was quite trivial in some ways, it set the stage for us to continue exploring the effectiveness of reward models and the effect of the seed in image generation. 
            </p>
          </div>
        </div>

        <div class="pub-entry">
          <div class="pub-img">
            <img src='imgs/egocvr.png' alt='EgoCVR publication image' height="100%" width="auto">
          </div>
          <div class="pub-info">
            <h3>EgoCVR: An Egocentric Benchmark for Fine-Grained Composed Video Retrieval</h3>
            <br>
            <a href="https://hummelth.github.io/">Thomas Hummel*</a>, <strong>Shyamgopal Karthik*</strong>, <a href="https://lilygeorgescu.github.io/">Mariana-Iuliana Georgescu</a>, and <a href="https://www.eml-unitue.de/people/zeynep-akata">Zeynep Akata</a><br>
            <em>ECCV 2024</em>, Milan, Italy.<br><a href="https://arxiv.org/abs/2407.16658"><i class="fa-solid fa-file-lines"></i>&nbsp;paper</a>&nbsp;|&nbsp;<a href="https://github.com/ExplainableML/EgoCVR"><i class="fa-brands fa-github"></i>&nbsp;code</a>&nbsp;|&nbsp;<a href="newbibs/egocvr.bib"><i class="fa-solid fa-book"></i>&nbsp;bibtex</a>
            <p>Building on our previous work, we were keen on exploring Composed Video Retrieval. The biggest issue was that the existing benchmark (WebVid-CoVR) was focused excessively on images and did not really require the whole video to solve the task. To address this issue, we spent a lot of time manually curating a nice evaluation set from Ego4D which eventually turned out into a very nice benchmark. CIReVL adapted for videos also turned out to be a very nice training-free method that was competitive with mehtods training on millions of videos!
            </p>
          </div>
        </div>

        <div class="pub-entry highlight">
          <div class="pub-img">
            <img src='imgs/CIReVL-Full.png' alt='Vision-by-Language publication image' height="100%" width="auto">
          </div>
          <div class="pub-info">
            <h3>Vision-by-Language for Training-Free Compositional Image Retrieval</h3>
            <br>
            <strong>Shyamgopal Karthik*</strong>,<a href="https://karroth.com/"> Karsten Roth*</a>, <a href="https://mancinimassimiliano.github.io/">Massimiliano Mancini</a>, and <a href="https://www.eml-unitue.de/people/zeynep-akata">Zeynep Akata</a><br>
            <em>ICLR 2024</em>, Vienna, Austria.<br><a href="https://arxiv.org/abs/2310.09291"><i class="fa-solid fa-file-lines"></i>&nbsp;paper</a>&nbsp;|&nbsp;<a href="https://github.com/ExplainableML/Vision_by_Language"><i class="fa-brands fa-github"></i>&nbsp;code</a>&nbsp;|&nbsp;<a href="newbibs/cirevl.bib"><i class="fa-solid fa-book"></i>&nbsp;bibtex</a>
            <p>We started off looking at Composed Image Retrieval task where we have a query image and textual instruction that modified the query. Popular methods for this task  were trained similar to textual inversion methods and predicted a "pseudo-token" for the query image. Our immediate instinct was that using an off-the-shelf captioning model <strong> must </strong> provide a stronger and more interpretable signal than these trained pseudo-token methods. Therefore, our "vision-by-language" method was just to caption an image, reformulate the caption based on the textual instruction and retrieve images based on the reformulated caption. Not only was this method more interpretable and training-free, it also allowed us to <strong>double</strong> the state-of-the-art performance on some popular benchmarks. 
            </p>
          </div>
        </div>

        <div class="pub-entry">
          <div class="pub-img">
            <img src='imgs/kgsp.png' alt='KG-SP publication image' height="100%" width="auto">
          </div>
          <div class="pub-info">
            <h3>KG-SP: Knowledge Guided Simple Primitives for Open World Compositional Zero-Shot Learning</h3>
            <br>
            <strong>Shyamgopal Karthik</strong>, <a href="https://mancinimassimiliano.github.io/">Massimiliano Mancini</a>, and <a href="https://www.eml-unitue.de/people/zeynep-akata">Zeynep Akata</a><br>
            <em>CVPR 2022</em>, New Orleans, USA.<br><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Karthik_KG-SP_Knowledge_Guided_Simple_Primitives_for_Open_World_Compositional_Zero-Shot_CVPR_2022_paper.pdf"><i class="fa-solid fa-file-lines"></i>&nbsp;paper</a>&nbsp;|&nbsp;<a href="https://github.com/ExplainableML/KG-SP"><i class="fa-brands fa-github"></i>&nbsp;code</a>&nbsp;|&nbsp;<a href="newbibs/kgsp.bib"><i class="fa-solid fa-book"></i>&nbsp;bibtex</a>
            <p>In this work, we looked at the problem of Compositional Zero-Shot Learning, where the goal is to predict (attribute, object) labels for an image, and generalize to unseen (attribute, object) pairs. Recent methods had tried to model attributes and objects jointly using a variety of ideas. Here, we show that predicting attributes and objects independently can work quite well for this task. Additionally, we show how a knowledge-base can be incorporated to improve the performance of the model at inference. Finally, we introduce a new partially labeled setting where we show how we can train our model in the absence of compositional labels. 
            </p>
          </div>
        </div>

        <div class="pub-entry">
          <div class="pub-img">
            <img src='imgs/ssl_big.png' alt='SSL publication image' width="auto" height="100%">
          </div>
          <div class="pub-info">
            <h3>Learning from Long-Tailed Data with Noisy Labels</h3>
            <br>
            <strong>Shyamgopal Karthik</strong>, <a href="https://europe.naverlabs.com/people_user/Jerome-Revaud/">Jerome Revaud</a>, and <a href="https://europe.naverlabs.com/people_user/Boris-Chidlovskii/">Boris Chidlovskii</a><br>
            <em>ICCV 2021 Workshop on Self-supervised Learning for Next-Generation Industry-level Autonomous Driving</em>, Virtual.<br><a href="https://arxiv.org/abs/2108.11096"><i class="fa-solid fa-file-lines"></i>&nbsp;paper</a>&nbsp;|&nbsp;<a href="newbibs/ssl_noise.bib"><i class="fa-solid fa-book"></i>&nbsp;bibtex</a>
            <p>This paper started off as a fun journey towards developing methods that are robust to both label noise and long-tailed class distributions. Methods tailored for one of these challenges collapsed when the other challenge was introduced. In the end, it turned out that vanilla self-supervised training went a long way in learning representations that were robust to both label noise and long-tailed distributions. 
            </p>
          </div>
        </div>

        <div class="pub-entry">
         <div class="pub-img">
            <img src='imgs/iclr.jpg' alt='ICLR publication image' width="200">
          </div>
          <div class="pub-info">
            <h3>No Cost Likelihood Manipulation at Test Time for Making Better Mistakes in Deep Networks</h3>
            <br>
            <strong>Shyamgopal Karthik</strong>, <a href="https://drimpossible.github.io/">Ameya Prabhu</a>, <a href="https://puneetkdokania.github.io/">Puneet Dokania</a>, and <a href="https://faculty.iiit.ac.in/~vgandhi/">Vineet Gandhi</a><br>
            <em> ICLR 2021</em>, Virtual.<br><a href="https://arxiv.org/abs/2104.00795"><i class="fa-solid fa-file-lines"></i>&nbsp;paper</a>&nbsp;|&nbsp;<a href="https://github.com/sgk98/CRM-Better-Mistakes"><i class="fa-brands fa-github"></i>&nbsp;code</a>&nbsp;|&nbsp;<a href="newbibs/iclr.bib"><i class="fa-solid fa-book"></i>&nbsp;bibtex</a>
             <p>The main motivation behind this work was to see if we could reduce the severity of mistakes in a classification setting. To do this, we make use of label hierarchies which are readily available through taxonomies like WordNet. For our method, we show that a simple algorithm from Duda and Hart's Pattern Recognition textbook way back in 1973 can be effectively used in a post-hoc manner while retaining the calibration of the base model. 
            </p>
          </div>
        </div>

        <div class="pub-entry">
          <div class="pub-img">
            <img src='imgs/tracking.jpg' alt='Tracking publication image' width="200">
          </div>
          <div class="pub-info">
            <h3>Simple Unsupervised Multi-Object Tracking</h3>
            <br>
            <strong>Shyamgopal Karthik</strong>, <a href="https://drimpossible.github.io">Ameya Prabhu</a>, and <a href="https://faculty.iiit.ac.in/~vgandhi">Vineet Gandhi</a> <br>
           <em> Arxiv 2020</em>. <br><a href="https://arxiv.org/abs/2006.02609"><i class="fa-solid fa-file-lines"></i>&nbsp;paper</a>&nbsp;|&nbsp;<a href="newbibs/mot.bib"><i class="fa-solid fa-book"></i>&nbsp;bibtex</a>
           <p>We revisited Re-Idenification models that were widely used in Multi-Object Tracking algorithms. In various trackers, this is often the only component that requires video level supervision. Our insight was that we could train a ReID model using pseudo-labels generated from a Kalman filter based tracker in a self-supervised fashion. The resulting ReID model can be used as a drop-in replacement to the supervised ReID models used in trackers. Alternatively, using these ReID features as a post-processing step in trackers that don't use a ReID model can reduce the number of ID Switches by 67%. In hindsight, I hope this was useful in motivating some <a href="https://arxiv.org/abs/2206.04656">works</a> later on which strengthened the traditional tracking-by-detection paradigm. 
           </p>
         </div>
       </div>
      </div>
    </div>
    <div style="margin-top: 40px;">
      <p align="right">
        <font size="2">
          Website style cloned from <a href="https://jonbarron.info/"><strong>this</strong></a> wonderful website.<br> Last update: May 2025
        </font>
      </p>
    </div>
  </div>
  </div>
  </body>
</html>

