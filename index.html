<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
  <head>
  <meta name=viewport content=“width=800”>
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
    /* Color scheme stolen from Sergey Karayev */
    a {
    color: #1772d0;
    text-decoration:none;
    }
    a:focus, a:hover {
    color: #f09228;
    text-decoration:none;
    }
    body,td,th,tr,p,a {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px
    }
    strong {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px;
    }
    heading {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 22px;
    }
    papertitle {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 16px;
    font-weight: 700
    }
    name {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 32px;
    }
    .one
    {
    width: 160px;
    height: 160px;
    position: relative;
    }
    .two
    {
    width: 160px;
    height: 160px;
    position: absolute;
    transition: opacity .2s ease-in-out;
    -moz-transition: opacity .2s ease-in-out;
    -webkit-transition: opacity .2s ease-in-out;
    }
    .fade {
     transition: opacity .2s ease-in-out;
     -moz-transition: opacity .2s ease-in-out;
     -webkit-transition: opacity .2s ease-in-out;
    }
    span.highlight {
        background-color: #ffffd0;
    }
  </style>
  <title>Shyamgopal Karthik</title>
  <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
  <link href='http://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
  </head>
  <body>
  <table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
    <tr>
    <td>
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="67%" valign="middle">
        <p align="center">
          <name>Shyamgopal Karthik</name>
        </p>
        <p>I am a PhD candidate at the <a href="https://eml-unitue.de/">Explainable Machine Learning</a> lab of University of Tuebingen, led by Prof. Zeynep Akata. I am always open to exploring new problems in the areas of computer vision and machine learning. In the past, I have worked on self-supervised learning, visual tracking, using class hierarchies to improve classification, audio-visual saliency, and active learning. However, I love the Occam's Razor and the kind of research that is able to provide simple and clear explanations to complex phenomena. This <a href="https://www.youtube.com/watch?v=kY2NHSKBi10">talk</a> accurately summarizes the kind of research I enjoy the most.
</p>
            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="0">
      <tr>
        <td width="100%" valign="middle">
          <heading>Bio</heading>
        </td>
      </tr>
      </table>
        <p>
          I completed my Master's and Bachelor's degree from the International Institute of Information Technology, Hyderabad in 2021 where I worked at the <a href="http://cvit.iiit.ac.in/">Center for Visual Information Technology</a> with <a href="https://faculty.iiit.ac.in/~vgandhi/">Prof. Vineet Gandhi</a>. During my Master's, I spent a wonderful 6 months remotely interning at <a href="https://europe.naverlabs.com/">NAVER LABS Europe</a> where I worked with <a href="https://europe.naverlabs.com/people_user/Boris-Chidlovskii/">Boris Chidlovskii</a> and <a href="https://europe.naverlabs.com/people_user/Jerome-Revaud/">Jerome Revaud</a> on learning from long-tailed and noisy data. 
        </p>
	<p>
		</p>
        <p align=center>
          <a href="mailto: shyamgopalkart@gmail.com">Email</a> &nbsp/&nbsp
          <a href="./pdfs/CV_Shyam.pdf">CV</a> &nbsp/&nbsp
          <a href="https://scholar.google.co.in/citations?user=OiVCfscAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
	  <a href="https://github.com/sgk98/">GitHub</a> &nbsp/&nbsp
            <a href="https://twitter.com/ShyamgopalKart1">Twitter</a>
        </p>
        </td>
        <td width="33%">
          <img width="100%" style="border-radius: 50%" src="./imgs/profile.jpeg">
        </td>
      </tr>
      </table>
      <table>
           <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Updates</heading>
              <p>
                <ul>
                  <li>03 July 2922. Our work on post-hoc uncertainty estimation was accepted to <a href="https://eccv2022.ecva.net/">ECCV 2022</a></li>
                  <li>21 May 2022. I was recognized as an outstanding reviewer at <a href="https://cvpr2022.thecvf.com/outstanding-reviewers">CVPR 2022</a>.</li>
                  <li>3 March 2022. Our work on Compositional Zero-Shot Learning was accepted at <a href="https://cvpr2022.thecvf.com/">CVPR 2022</a></li>
                  <li>24 November 2021 . I was recognized as an outstanding reviewer at <a href="https://www.bmvc2021-virtualconference.com/">BMVC 2021</a>.</li>
                  <li>1 October 2021. Started my PhD at the <a href="https://www.eml-unitue.de/"> Explainable Machine Learning</a> group. </li>
                  <li>11 July 2021. Concluded my internship at NAVER LABS Europe</li>
                  <li>7 April 2021. Successfully defended my Masters Thesis.</li>
                </ul>
              </p>
            </td>
      </table>






      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="100%" valign="middle">
          <heading>Publications</heading>
        </td>
      </tr>
      </table>
        <p>
	2022
	</p>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="25%" >
          <img width="200" height="125" src='imgs/BayesCap.gif'>
        </td>
        <td valign="top" width="75%">
          <p>
              <papertitle>BayesCap: Bayesian Identity Cap for Calibrated
Uncertainty in Frozen Neural Networks
</papertitle>
            <br>
            <a href="https://udion.github.io">Uddeshya Upadhyay*</a>, <strong>Shyamgopal Karthik*</strong>, <a href="https://yanbeic.github.io/">Yanbei Chen</a>, <a href="https://mancinimassimiliano.github.io/">Massimiliano Mancini</a>, and <a href="https://www.eml-unitue.de/people/zeynep-akata">Zeynep Akata</a><br>
            <em>ECCV 2022</em>, Tel-Aviv, Israel. <br> <a href="https://arxiv.org/pdf/2207.06873.pdf"> paper</a> <a href="https://github.com/ExplainableML/BayesCap"> code</a> <a href="newbibs/bayescap.bib"> bibtex</a>
            <p>We provide "free" uncertainty estimates to your favourite image-translation model. The key idea is that state-of-the-art image translation models are deterministic, whereas probabilistic models are much harder to train. Our idea is to train a probabilistic model in a post-hoc fashion which provides calibrated uncertainty estimates that can be used in downstream tasks like detecting out-of-distribution samples in safety-critical scenarios like depth estimation for autonomous driving.   
            </p>
          </p>
        </td>
      </tr>
</table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="25%">
          <img width="200" src='imgs/kgsp.png'>
        </td>
        <td valign="top" width="75%" >
          <p>
              <papertitle>KG-SP: Knowledge Guided Simple Primitives for Open World Compositional Zero-Shot Learning 
</papertitle>
            <br>
            <strong>Shyamgopal Karthik</strong>, <a href="https://mancinimassimiliano.github.io/">Massimiliano Mancini</a> and <a href="https://www.eml-unitue.de/people/zeynep-akata">Zeynep Akata</a><br>
            <em>CVPR 2022</em>, New Orleans, USA.<br><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Karthik_KG-SP_Knowledge_Guided_Simple_Primitives_for_Open_World_Compositional_Zero-Shot_CVPR_2022_paper.pdf"> paper</a> <a href="https://github.com/ExplainableML/KG-SP">code</a><a href="newbibs/kgsp.bib"> bibtex</a>
            <p>In this work, we looked at the problem of Compositional Zero-Shot Learning, where the goal is to predict (attribute, object) labels for an image, and generalize to unseen (attribute, object) pairs. Recent methods had tried to model attributes and objects jointly using a variety of ideas. Here, we show that predicting attributes and objects independently can work quite well for this task. Additionally, we show how a knowledge-base can be incorporated to improve the performance of the model. Finally, we introduce a new partially labeled setting where we show how we can train our model in the absence of compositional labels. 
            </p>
          </p>
        </td>
      </tr>
</table>
  <p>
  2021
  </p>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="25%">
          <img width="200" src='imgs/three_generalization.png'>
        </td>
        <td valign="top" width="75%">
          <p>
              <papertitle>Bringing Generalization to Deep Multi-View Detection
</papertitle>
            <br>
            <a href="https://www.linkedin.com/in/jeetvora/">Jeet Vora</a>, <a href="https://www.linkedin.com/in/swetanjal/">Swetanjal Dutta</a>, <a href="https://scholar.google.com/citations?user=NCVuTTAAAAAJ&hl=en">Kanishk Jain</a>, <strong>Shyamgopal Karthik</strong> and <a href="https://faculty.iiit.ac.in/~vgandhi/">Vineet Gandhi</a><br>
            <em>Arxiv</em>. <br> <a href="pdfs/MVD.pdf"> paper</a> <a href="https://github.com/jeetv/GMVD"> code</a> <a href="newbibs/mvd.bib"> bibtex</a>
            <p>Here, we tackle the problem of multi-view detection where we want to generate the bird's eye view map of a scene given multiple views. The biggest challenge here is that capturing training data is quite difficult since it requires calibrated and synchronized cameras. We sidestep this problem by generating a large-scale training data using GTA-V and Unity graphics engines. We also propose some simple modifications to existing models and show that training on our synthetic dataset generalizes quite well on real data. 
            </p>
          </p>
        </td>
      </tr>
</table>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="25%">
          <img width="200" src='imgs/ssl_big.png'>
        </td>
        <td valign="top" width="75%">
          <p>
              <papertitle>Learning from Long-Tailed Data with Noisy Labels
</papertitle>
            <br>
            <strong>Shyamgopal Karthik</strong>, <a href="https://europe.naverlabs.com/people_user/Jerome-Revaud/">Jerome Revaud</a> and <a href="https://europe.naverlabs.com/people_user/Boris-Chidlovskii/">Boris Chidlovskii</a><br>
            <em>ICCV 2021 Workshop on Self-supervised Learning for Next-Generation Industry-level Autonomous Driving</em>, Virtual.<br><a href="https://arxiv.org/pdf/2108.11096.pdf"> paper</a> <a href="newbibs/ssl_noise.bib"> bibtex</a>
            <p>This work brings together self-supervised learning, long-tailed learning and learning with noisy labels. An important thing we noticed was that long-tailed methods break down when trained with noisy labels and vice-versa. We found that self-supervised pre-training followed by fine-tuning with a robust loss function which handles both imbalance and label noise works exceptionally well on a bunch of datasets. 
            </p>
          </p>
        </td>
      </tr>
</table>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="25%">
          <img width="200" src='imgs/DHF1K_results1.jpg'>
        </td>
        <td valign="top" width="75%">
          <p>
              <papertitle>ViNet: Pushing the limits of Visual Modality for Audio-Visual Saliency Prediction
</papertitle>
            <br>
            <a href="https://www.linkedin.com/in/samyak-jain-ab4038165/">Samyak Jain</a>, <a href="https://www.linkedin.com/in/pradeep-yarlagadda-b043156b/">Pradeep Yarlagadda</a>, <a href="https://www.linkedin.com/in/shreyank-jyoti-a9b5b37b/">Shreyank Jyoti</a>, <strong>Shyamgopal Karthik</strong>, <a href="https://sites.google.com/site/raamsubram/">Ramanathan Subramanian</a> and <a href="https://faculty.iiit.ac.in/~vgandhi/">Vineet Gandhi</a><br>
            <em>IROS 2021</em>, Virtual. <br><a href="https://arxiv.org/pdf/2012.06170.pdf"> paper</a> <a href="https://github.com/samyak0210/ViNet"> code</a><a href="newbibs/saliency.bib"> bibtex</a>
            <p>We started off with a 3D-CNN architecture which achieves State-of-the-Art performance on various video saliency benchmarks. We then incorporated audio into our model, and achieved excellent performance on audio-visual saliency benchmarks as well. However, a very curious observation we noticed was that the output remains unchanged even if we proivde zeros for the audio indicating that the audio is being ignored entirely in various State-of-the-Art models. 
            </p>
          </p>
        </td>
      </tr>
</table>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
       <td width="25%" bgcolor="#ffffd0">
          <img width="200" src='imgs/iclr.jpg'>
        </td>
        <td valign="top" width="75%" bgcolor="#ffffd0">
          <p>
              <papertitle>No Cost Likelihood Manipulation at Test Time for Making Better Mistakes in Deep Networks
</papertitle>
            <br>
            <strong>Shyamgopal Karthik</strong>, <a href="https://drimpossible.github.io/">Ameya Prabhu</a>, <a href="https://puneetkdokania.github.io/">Puneet Dokania</a> and <a href="https://faculty.iiit.ac.in/~vgandhi/">Vineet Gandhi</a><br>
            <em> ICLR 2021</em>, Virtual.<br><a href="https://arxiv.org/pdf/2104.00795.pdf"> paper</a><a href="https://github.com/sgk98/CRM-Better-Mistakes"> code</a> <a href="newbibs/iclr.bib"> bibtex</a>
             <p>The main motivation behind this work was to see if we could reduce the severity of mistakes in a classification setting. To do this, we make use of label hierarchies which are readily available through taxonomies like WordNet. For our method, we show that a simple algorithm from Duda and Hart's Pattern Recognition textbook way back in 1973 can be effectively used in a post-hoc manner while retaining the calibration of the base model. 
            </p>
          </p>
        </td>
      </tr>
</table>
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="25%">
          <img width="200" src='imgs/shyam_thesis.png'>
        </td>
        <td valign="top" width="75%">
          <p>
              <papertitle>Leveraging Structural Cues for Better Training and Deployment in Computer Vision
</papertitle>
            <br>
            <strong> Shyamgopal Karthik</strong><br>
            <em> Master's  Thesis</em>, IIIT Hyderabad <br><a href="http://cdn.iiit.ac.in/cdn/cvit.iiit.ac.in/images/Thesis/MS/Shyamgopal/Shyamgopal_Thesis.pdf"> thesis</a> <a href="newbibs/thesis.bib"> bibtex</a>
            <p>My Master's thesis which was the culmination of nearly 3 years of work from my side!
            </p>
          </p>
        </td>
      </tr>
</table>
	<p>
	2020
	</p>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="25%">
          <img width="200" src='imgs/tracking.jpg'>
        </td>
        <td valign="top" width="75%">
          <p>
              <papertitle>Simple Unsupervised Multi-Object Tracking
</papertitle>
            <br>
            <strong>Shyamgopal Karthik</strong>, <a href="https://drimpossible.github.io">Ameya Prabhu</a> and <a href="https://faculty.iiit.ac.in/~vgandhi">Vineet Gandhi</a> <br>
           <em> Arxiv</em>. <br><a href="https://arxiv.org/pdf/2006.02609.pdf"> paper</a> <a href="newbibs/mot.bib"> bibtex</a>
           <p>This work mainly focuses on the Re-Identification features that are commonly used in Multi-Object Tracking algorithms. In various trackers, this is often the only component that requires video level supervision. We propose a method to train a ReID model using pseudo-labels generated from a Kalman filter based tracker in a self-supervised fashion. The resulting ReID model can be used as a drop-in replacement to the supervised ReID models used in trackers. Alternatively, using these ReID features as a post-processing step in trackers that don't use a ReID model can reduce the number of ID Switches by 67%.  
           </p>
         </p>
        </td>
      </tr>
</table>

                             <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="25%">
          <img width="200" src='imgs/3rs.jpg'>
        </td>
        <td valign="top" width="75%">
          <p>
              <papertitle>Exploring 3 R's of Long-term Tracking: Re-detection, Recovery and Reliability
</papertitle>
            <br>
            <strong> Shyamgopal Karthik</strong>, <a href="https://amoudgl.github.io">Abhinav Moudgil</a> and <a href="http://faculty.iiit.ac.in/~vgandhi">Vineet Gandhi</a><br>
            <em> IEEE WACV 2020</em>.<br><a href="https://openaccess.thecvf.com/content_WACV_2020/papers/Karthik_Exploring_3_Rs_of_Long-term_Tracking_Redetection_Recovery_and_Reliability_WACV_2020_paper.pdf"> paper</a> <a href="newbibs/wacv.bib"> bibtex</a>
            <p>My first work which introduced me to the field of computer vision! We found some interesting quirks and shortcomings of existing State-of-the-Art tracking algorithms when evaluated on long-term visual object tracking datasets. We did our best to propose novel  experiments and evaluation metrics to highlight these quirks. 
            </p>
             </p>
        </td>
      </tr>
</table>

	<p>

        <br>
        <p align="right">
          <font size="2">
          Website style cloned from <a href="https://jonbarron.info/"><strong>this</strong></a> wonderful website.<br> Last update: July 2022
	    </font>
        </p>
        </td>
      </tr>
      </table>
    </td>
    </tr>
  </table>
  </body>
</html>

